{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PocketSage Model Evaluation - Reproducible Inference Notebook\n",
        "\n",
        "This notebook provides a complete, reproducible evaluation pipeline for the PocketSage receipt categorization model.\n",
        "\n",
        "## Contents\n",
        "1. Setup and Configuration\n",
        "2. Data Loading and Normalization\n",
        "3. Model Evaluation\n",
        "4. Confusion Matrix Analysis\n",
        "5. Per-Category Metrics\n",
        "6. Inference Examples\n",
        "7. Results Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.append(str(project_root))\n",
        "sys.path.append(str(project_root / 'api-endpoints'))\n",
        "sys.path.append(str(project_root / 'evaluation'))\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Import custom modules\n",
        "from normalization import ReceiptNormalizer\n",
        "from gemini_retraining import GeminiReceiptTrainer, load_receipts_from_firestore\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config_path = project_root / 'evaluation' / 'fine_tuning_config.yaml'\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  Model: {config['model']['base_model']}\")\n",
        "print(f\"  Categories: {len(config['categories']['categories'])}\")\n",
        "print(f\"  Normalization: {config['normalization']['enabled']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize normalizer and trainer\n",
        "normalizer = ReceiptNormalizer()\n",
        "trainer = GeminiReceiptTrainer(model_name=config['model']['base_model'])\n",
        "\n",
        "# Load receipt data\n",
        "print(\"Loading receipt data...\")\n",
        "all_receipts = load_receipts_from_firestore(limit=100)\n",
        "print(f\"Loaded {len(all_receipts)} receipts\")\n",
        "\n",
        "# Prepare training data\n",
        "training_examples = trainer.prepare_training_data(all_receipts)\n",
        "print(f\"Prepared {len(training_examples)} training examples\")\n",
        "\n",
        "# If insufficient data, use mock examples\n",
        "if len(training_examples) < 2:\n",
        "    print(\"Using mock examples for demonstration...\")\n",
        "    training_examples = [\n",
        "        {\n",
        "            'input': 'Vendor: Grocery Store\\nTotal: 150.50\\nItems:\\n  - Milk: 50\\n  - Bread: 30',\n",
        "            'output': 'groceries',\n",
        "            'metadata': {}\n",
        "        },\n",
        "        {\n",
        "            'input': 'Vendor: Restaurant\\nTotal: 500\\nItems:\\n  - Pizza: 300\\n  - Drinks: 200',\n",
        "            'output': 'dining',\n",
        "            'metadata': {}\n",
        "        },\n",
        "        {\n",
        "            'input': 'Vendor: Gas Station\\nTotal: 2000\\nItems:\\n  - Fuel: 2000',\n",
        "            'output': 'transportation',\n",
        "            'metadata': {}\n",
        "        },\n",
        "        {\n",
        "            'input': 'Vendor: Electricity Board\\nTotal: 1500\\nItems:\\n  - Electricity Bill: 1500',\n",
        "            'output': 'utilities',\n",
        "            'metadata': {}\n",
        "        },\n",
        "        {\n",
        "            'input': 'Vendor: Hotel\\nTotal: 5000\\nItems:\\n  - Room: 5000',\n",
        "            'output': 'travel',\n",
        "            'metadata': {}\n",
        "        },\n",
        "    ]\n",
        "\n",
        "# Split into training and test sets\n",
        "train_split = config['training']['train_split']\n",
        "split_idx = int(len(training_examples) * train_split)\n",
        "train_set = training_examples[:split_idx]\n",
        "test_set = training_examples[split_idx:]\n",
        "\n",
        "if len(test_set) == 0:\n",
        "    test_set = train_set[:min(5, len(train_set))]\n",
        "\n",
        "print(f\"\\nTraining set: {len(train_set)} examples\")\n",
        "print(f\"Test set: {len(test_set)} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "print(\"Starting model evaluation...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "metrics = trainer.evaluate_model(\n",
        "    training_data=train_set,\n",
        "    test_data=test_set,\n",
        "    use_few_shot=True\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluation complete!\")\n",
        "\n",
        "# Display overall metrics\n",
        "overall = metrics['overall']\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"OVERALL METRICS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Accuracy:  {overall['accuracy']:.4f} ({overall['accuracy']*100:.2f}%)\")\n",
        "print(f\"Precision: {overall['precision']:.4f} ({overall['precision']*100:.2f}%)\")\n",
        "print(f\"Recall:    {overall['recall']:.4f} ({overall['recall']*100:.2f}%)\")\n",
        "print(f\"F1 Score:  {overall['f1_score']:.4f} ({overall['f1_score']*100:.2f}%)\")\n",
        "\n",
        "# Compare with targets\n",
        "target_f1 = config['evaluation']['target_metrics']['macro_f1_score']\n",
        "print(f\"\\nTarget F1 Score: {target_f1:.4f}\")\n",
        "print(f\"Target Met: {'✓ YES' if overall['f1_score'] >= target_f1 else '✗ NO'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Confusion Matrix Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate confusion matrix visualization\n",
        "cm = np.array(metrics['confusion_matrix'])\n",
        "categories = trainer.categories\n",
        "\n",
        "# Normalized confusion matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm_normalized = np.nan_to_num(cm_normalized)\n",
        "\n",
        "# Create visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Normalized heatmap\n",
        "sns.heatmap(\n",
        "    cm_normalized,\n",
        "    annot=True,\n",
        "    fmt='.2%',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[cat[:10] for cat in categories],\n",
        "    yticklabels=[cat[:10] for cat in categories],\n",
        "    ax=ax1,\n",
        "    cbar_kws={'label': 'Percentage'}\n",
        ")\n",
        "ax1.set_xlabel('Predicted Category', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('True Category', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Raw counts heatmap\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[cat[:10] for cat in categories],\n",
        "    yticklabels=[cat[:10] for cat in categories],\n",
        "    ax=ax2,\n",
        "    cbar_kws={'label': 'Count'}\n",
        ")\n",
        "ax2.set_xlabel('Predicted Category', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('True Category', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Confusion Matrix (Raw Counts)', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Per-Category Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create per-category metrics DataFrame\n",
        "per_category = metrics['per_category']\n",
        "df_metrics = pd.DataFrame([\n",
        "    {\n",
        "        'Category': cat,\n",
        "        'Precision': vals['precision'],\n",
        "        'Recall': vals['recall'],\n",
        "        'F1 Score': vals['f1_score']\n",
        "    }\n",
        "    for cat, vals in per_category.items()\n",
        "])\n",
        "\n",
        "print(\"Per-Category Metrics:\")\n",
        "print(\"=\" * 80)\n",
        "print(df_metrics.to_string(index=False))\n",
        "\n",
        "# Check which categories meet the target\n",
        "min_f1 = config['evaluation']['target_metrics']['per_category_f1_min']\n",
        "df_metrics['Meets Target'] = df_metrics['F1 Score'] >= min_f1\n",
        "print(f\"\\nCategories meeting F1 ≥ {min_f1}: {df_metrics['Meets Target'].sum()}/{len(df_metrics)}\")\n",
        "\n",
        "# Visualize per-category metrics\n",
        "categories_list = df_metrics['Category'].values\n",
        "precision_vals = df_metrics['Precision'].values\n",
        "recall_vals = df_metrics['Recall'].values\n",
        "f1_vals = df_metrics['F1 Score'].values\n",
        "\n",
        "x = np.arange(len(categories_list))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "bars1 = ax.bar(x - width, precision_vals, width, label='Precision', \n",
        "              color='#3498db', alpha=0.8, edgecolor='black')\n",
        "bars2 = ax.bar(x, recall_vals, width, label='Recall', \n",
        "              color='#2ecc71', alpha=0.8, edgecolor='black')\n",
        "bars3 = ax.bar(x + width, f1_vals, width, label='F1 Score', \n",
        "              color='#e74c3c', alpha=0.8, edgecolor='black')\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2, bars3]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if height > 0.01:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.2f}',\n",
        "                   ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Add target line\n",
        "target = config['evaluation']['target_metrics']['per_category_f1_min']\n",
        "ax.axhline(y=target, color='red', linestyle='--', linewidth=2, \n",
        "          label=f'Target F1: {target}', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Category', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Per-Category Performance Metrics', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([cat[:12] for cat in categories_list], rotation=45, ha='right')\n",
        "ax.set_ylim([0, 1.1])\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test inference on sample receipts\n",
        "print(\"Testing inference on sample receipts...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sample_receipts = [\n",
        "    {\n",
        "        'parsedData': {\n",
        "            'raw': {\n",
        "                'vendor': 'DMART',\n",
        "                'total': 450.50,\n",
        "                'items': [\n",
        "                    {'name': 'MILK 2%', 'price': 50},\n",
        "                    {'name': 'BRD LOAF', 'price': 30},\n",
        "                    {'name': 'EGGS', 'price': 70.50},\n",
        "                    {'name': 'RICE 5KG', 'price': 300}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'parsedData': {\n",
        "            'raw': {\n",
        "                'vendor': 'SWIGGY',\n",
        "                'total': 350,\n",
        "                'items': [\n",
        "                    {'name': 'PIZZA', 'price': 200},\n",
        "                    {'name': 'COKE', 'price': 50},\n",
        "                    {'name': 'DELIVERY', 'price': 100}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'parsedData': {\n",
        "            'raw': {\n",
        "                'vendor': 'PETROL PUMP',\n",
        "                'total': 2000,\n",
        "                'items': [\n",
        "                    {'name': 'PETROL', 'price': 2000}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, receipt in enumerate(sample_receipts, 1):\n",
        "    predicted = trainer.train_with_few_shot(train_set, receipt)\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"  Vendor: {receipt['parsedData']['raw']['vendor']}\")\n",
        "    print(f\"  Total: ₹{receipt['parsedData']['raw']['total']}\")\n",
        "    print(f\"  Predicted Category: {predicted}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print comprehensive summary\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nModel: {metrics['model_name']}\")\n",
        "print(f\"Timestamp: {metrics['timestamp']}\")\n",
        "print(f\"Test Set Size: {metrics['test_size']}\")\n",
        "print(f\"Training Examples: {metrics['training_size']}\")\n",
        "\n",
        "print(\"\\nOverall Performance:\")\n",
        "print(f\"  Accuracy:  {overall['accuracy']:.4f} ({overall['accuracy']*100:.2f}%)\")\n",
        "print(f\"  Precision: {overall['precision']:.4f} ({overall['precision']*100:.2f}%)\")\n",
        "print(f\"  Recall:    {overall['recall']:.4f} ({overall['recall']*100:.2f}%)\")\n",
        "print(f\"  F1 Score:  {overall['f1_score']:.4f} ({overall['f1_score']*100:.2f}%)\")\n",
        "\n",
        "target_f1 = config['evaluation']['target_metrics']['macro_f1_score']\n",
        "target_met = overall['f1_score'] >= target_f1\n",
        "print(f\"\\nTarget F1 Score: {target_f1:.4f}\")\n",
        "print(f\"Target Met: {'✓ YES' if target_met else '✗ NO'}\")\n",
        "\n",
        "min_f1 = config['evaluation']['target_metrics']['per_category_f1_min']\n",
        "categories_meeting_target = sum(\n",
        "    1 for cat, vals in per_category.items()\n",
        "    if vals['f1_score'] >= min_f1\n",
        ")\n",
        "total_categories = len(per_category)\n",
        "print(f\"\\nCategories meeting F1 ≥ {min_f1}: {categories_meeting_target}/{total_categories}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Evaluation complete!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save results\n",
        "output_dir = project_root / 'evaluation' / 'results'\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "results_file = output_dir / f\"pocketsage_evaluation_{timestamp}.json\"\n",
        "\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(f\"\\nResults saved to: {results_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
